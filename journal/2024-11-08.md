# November 7, 2024 Log Record

The captain's logbook date is November 8, 2024 03:25.

Today, I aimed to complete and train the GPT model with ChatGPT given instructions.

ChatGPT o1-preview generated the gpt.py file for me, but after reaching the limit, I switched to GPT-4o. The problems were solved (not too much), and then training started. 

Yesterday, I used 2 CPU Cores and 20 GB RAM for my virtual machine. The cooling fan shouted louder and louder until I completed the training. Today, I used 8 CPU Cores and 20 GB RAM. The cooling fan is steady so far.

After completing the process, I asked some extra questions to ChatGPT for adding 2 more models to the project. There will be 2 new models DistilBERT and T5. By the way, I realized to check the code files, ChatGPT only used the self-trained tokenizer in the GPT model. But in the Bert model, a pre-trained tokenizer. On the next coding day, I will update and re-train the models with a self-trained tokenizer and multiple files.

Next step

- Move the general information under the `doc` folder.
- Update the tokenizer in `bert.py`.
- Add to more models.
- Add multiple textbook.
- Seperate the duplicated codes and merge them in a single code file.
- 
## Today's Result

The GPT model has been trained with the Romeo and Juliet textbook. The result metrics are below. Again, I don't understand but I will learn them in detail.

```json
{'train_runtime': 684.2403, 'train_samples_per_second': 1.263, 'train_steps_per_second': 0.316, 'train_loss': 6.50891508879485, 'epoch': 3.0} 

# Yesterday BERT's Results
{'train_runtime': 765.0489, 'train_samples_per_second': 1.129, 'train_steps_per_second': 0.282, 'train_loss': 6.435161413969817, 'epoch': 3.0} 
```

When I compared the results, I saw with my ignorant eyes, that they were near the same. But I don't know am I right or wrong. ðŸ™ƒ

## Parameter Calculation

When calculating the number of parameters for a transformer-based model like BERT or GPT, key factors include:

- hidden_size: The size of the hidden layers.
- num_hidden_layers: The number of layers in the transformer model.
- num_attention_heads: The number of attention heads in each layer.
- intermediate_size (BERT-specific): The size of the intermediate (feed-forward) layers.
- vocab_size: The size of the modelâ€™s vocabulary.

### General Parameter Calculation Formula

#### Embedding Layer

The embedding layer parameter count is calculated as `vocab_size * hidden_size`.

#### Self-Attention Parameters (Per Attention Head)

- For each multi-head attention layer, the parameter count includes
    - Query, Key, and Value projection: `3 * (hidden_size * hidden_size) = 3 * hidden_size^2`
    - Output projection: `hidden_size * hidden_size`
- Total: `4 * hidden_size^2`

#### Intermediate Feed-Forward Layer (specific to BERT)

Each fully connected layer in the intermediate feed-forward network has parameters calculated as: `hidden_size * intermediate_size + intermediate_size * hidden_size = 2 * hidden_size * intermediate_size`

#### Total Parameters in a Layer

For each layer: `attention parameters + intermediate parameters`

#### Total Model Parameters

`(number of layers) * (parameters per layer) + Embedding Layer parameters`

According to the above formulas my models' parameter calculation would be like that

### BERT Model Parameter Calculation

#### Configuration

- ***hidden_size*** = 768
- ***num_hidden_layers*** = 12
- ***num_attention_heads*** = 12
- ***intermediate_size*** = 3072
- ***vocab_size*** = 30522

#### Calculations

- **Embedding Layer** = `vocab_size * hidden_size` = 30522 * 768 = 23475456
- **Self-Attention Parameters (Per Layer)** = `4 * hidden_size^2` = 4 * 768^2 = 2359296
- **Intermediate Feed-Forward Layer (Per Layer)** = `2 * hidden_size * intermediate_size` = 2 * 768 * 3072 = 4718592
- **Total Parameters Per Layer** = `Attention + Intermediate` = 2359296 + 4718592 = 7077888
- **Total for All Layers** = 12 * 7077888 = 84934656
- **Overall Model Parameters** = `Embedding + Layers` = 23475456 + 84934656 = 108410112

As a result my model has approximately 110 Million parameters like Bert-Base model.

### GPT Model Parameter Calculation

#### Configuration

- ***hidden_size*** = 768
- ***num_hidden_layers*** = 12
- ***num_attention_heads*** = 12
- ***vocab_size*** = 50257

#### Calculations

- **Embedding Layer** = `vocab_size * hidden_size` = 50257 * 768 = 38165376
- **Self-Attention Parameters (Per Layer)** = `4 * hidden_size^2` = 4 * 768^2 = 2359296
- **Total Parameters Per Layer** = `4 * hidden_size^2` = 2359296
- **Total for All Layers** = 12 * 2359296 = 28311552
- **Overall Model Parameters** = `Embedding + Layers` = 38165376 + 28311552 = 66476928

As a result my model has approximately 66 Million parameters like GPT-2 Small model.

### Key Factors Affecting Parameter Count

I also asked ChatGPT to how to increase my models parameters.

- ***hidden_size:*** Increasing this value raises the size of each hidden layer, thus increasing the overall parameter count significantly.
- ***num_hidden_layers:*** Adding more layers directly increases the number of parameters proportionally. Doubling the layers roughly doubles the parameters.
- ***intermediate_size (BERT):*** This affects the size of the intermediate feed-forward layer, increasing parameters for each layer.
- ***num_attention_heads:*** More attention heads mean additional parameters in the attention mechanism.